{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('data/GE.csv')\n",
    "df = dataframe[['Date', 'Close']]\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.lineplot(x=df['Date'], y=df['Close'])\n",
    "\n",
    "print(\"Start date is: \", df['Date'].min())\n",
    "print(\"End date is: \", df['Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Change train data from Mid 2017 to 2019.... seems to be a jump early 2017\n",
    "train, test = df.loc[df['Date'] <= '2003-12-31'], df.loc[df['Date'] > '2003-12-31']\n",
    "\n",
    "\n",
    "#Convert pandas dataframe to numpy array\n",
    "#dataset = dataframe.values\n",
    "#dataset = dataset.astype('float32') #COnvert values to float\n",
    "\n",
    "#LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "#scaler = MinMaxScaler() #Also try QuantileTransformer\n",
    "scaler = StandardScaler()\n",
    "scaler = scaler.fit(train[['Close']])\n",
    "\n",
    "train['Close'] = scaler.transform(train[['Close']])\n",
    "test['Close'] = scaler.transform(test[['Close']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#As required for LSTM networks, we require to reshape an input data into n_samples x timesteps x n_features. \n",
    "#In this example, the n_features is 2. We will make timesteps = 3. \n",
    "#With this, the resultant n_samples is 5 (as the input data has 9 rows).\n",
    "\n",
    "seq_size = 30  # Number of time steps to look back \n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "\n",
    "def to_sequences(x, y, seq_size=1):\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "\n",
    "    for i in range(len(x)-seq_size):\n",
    "        #print(i)\n",
    "        x_values.append(x.iloc[i:(i+seq_size)].values)\n",
    "        y_values.append(y.iloc[i+seq_size])\n",
    "        \n",
    "    return np.array(x_values), np.array(y_values)\n",
    "\n",
    "trainX, trainY = to_sequences(train[['Close']], train['Close'], seq_size)\n",
    "testX, testY = to_sequences(test[['Close']], test['Close'], seq_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define Autoencoder model\n",
    "#Input shape would be seq_size, 1 - 1 beacuse we have 1 feature. \n",
    "# seq_size = trainX.shape[1]\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(128, activation='relu', input_shape=(trainX.shape[1], trainX.shape[2]), return_sequences=True))\n",
    "# model.add(LSTM(64, activation='relu', return_sequences=False))\n",
    "# model.add(RepeatVector(trainX.shape[1]))\n",
    "# model.add(LSTM(64, activation='relu', return_sequences=True))\n",
    "# model.add(LSTM(128, activation='relu', return_sequences=True))\n",
    "# model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse')\n",
    "# model.summary()\n",
    "\n",
    "#Try another model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "model.add(Dropout(rate=0.2))\n",
    "\n",
    "model.add(RepeatVector(trainX.shape[1]))\n",
    "\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(TimeDistributed(Dense(trainX.shape[2])))\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "#model.evaluate(testX, testY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "#Anomaly is where reconstruction error is large.\n",
    "#We can define this value beyond which we call anomaly.\n",
    "#Let us look at MAE in training prediction\n",
    "\n",
    "trainPredict = model.predict(trainX)\n",
    "trainMAE = np.mean(np.abs(trainPredict - trainX), axis=1)\n",
    "plt.hist(trainMAE, bins=30)\n",
    "max_trainMAE = 0.3  #or Define 90% value of max as threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict = model.predict(testX)\n",
    "testMAE = np.mean(np.abs(testPredict - testX), axis=1)\n",
    "plt.hist(testMAE, bins=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture all details in a DataFrame for easy plotting\n",
    "anomaly_df = pd.DataFrame(test[seq_size:])\n",
    "anomaly_df['testMAE'] = testMAE\n",
    "anomaly_df['max_trainMAE'] = max_trainMAE\n",
    "anomaly_df['anomaly'] = anomaly_df['testMAE'] > anomaly_df['max_trainMAE']\n",
    "anomaly_df['Close'] = test[seq_size:]['Close']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot testMAE vs max_trainMAE\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['testMAE'])\n",
    "sns.lineplot(x=anomaly_df['Date'], y=anomaly_df['max_trainMAE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anomalies = anomaly_df.loc[anomaly_df['anomaly'] == True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Plot anomalies\n",
    "sns.lineplot(x=anomaly_df['Date'], y=scaler.inverse_transform(anomaly_df['Close']))\n",
    "sns.scatterplot(x=anomalies['Date'], y=scaler.inverse_transform(anomalies['Close']), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('Vib.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "441ddbc04b489d2063928333e0c797ff7d144f21a88cfddd13e19b4c1bc4e060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
